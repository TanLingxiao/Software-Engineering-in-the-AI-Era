# LLM 基础知识

## 什么是大型语言模型（LLM）？

大型语言模型（Large Language Model，简称 LLM）是一种基于深度学习的 AI 模型，通过在海量文本数据上进行预训练，学习语言的统计规律和语义关系。

### 核心特征

| 特征 | 描述 |
|------|------|
| **大规模参数** | 模型参数量通常在数十亿到数万亿级别 |
| **预训练** | 在大规模语料库上进行无监督学习 |
| **生成能力** | 能够生成连贯、有逻辑的文本 |
| **上下文理解** | 理解长文本中的上下文关系 |
| **多任务能力** | 无需专门训练即可处理多种任务 |

## LLM 的工作原理

### 1. 预训练阶段

```
海量文本数据 → Tokenization → 模型训练 → 预训练模型
```

预训练过程：
- **数据收集**：从互联网、书籍、代码库等来源收集文本
- **分词（Tokenization）**：将文本转换为数字序列
- **自监督学习**：模型学习预测下一个词或填充缺失内容
- **参数优化**：通过反向传播调整模型参数

### 2. 架构类型

#### Transformer 架构

Transformer 是现代 LLM 的基础架构，具有以下特点：

- **自注意力机制**：捕捉文本中的长距离依赖关系
- **并行计算**：提高训练和推理效率
- **位置编码**：理解词序信息

```
输入嵌入 → 位置编码 → 多层 Transformer → 输出
```

### 3. 推理过程

当用户向 LLM 提问时：

1. **输入处理**：将用户输入转换为 token 序列
2. **上下文编码**：模型理解输入的语义和上下文
3. **生成预测**：基于概率分布预测下一个 token
4. **迭代生成**：重复步骤 3 直到完成响应

## 主流 LLM 模型

| 模型 | 开发者 | 特点 |
|------|--------|------|
| **GPT-4** | OpenAI | 强大的推理和编程能力 |
| **Claude** | Anthropic | 安全性和长上下文处理 |
| **Gemini** | Google | 多模态能力 |
| **Llama** | Meta | 开源、可本地部署 |

## LLM 在软件开发中的应用

### 代码生成

```python
# 示例：使用 LLM 生成代码
# 提示：写一个 Python 函数来计算斐波那契数列

def fibonacci(n):
    if n <= 1:
        return n
    return fibonacci(n-1) + fibonacci(n-2)
```

### 代码理解

LLM 可以：
- 解释复杂代码的功能
- 生成代码文档
- 识别代码中的潜在问题

### 调试辅助

- 分析错误信息
- 提供修复建议
- 解释错误原因

## LLM 的局限性

### 幻觉（Hallucination）

模型可能会生成看似合理但不正确的内容。

**应对策略**：
- 要求模型提供推理过程
- 验证关键信息
- 使用 RAG（检索增强生成）技术

### 上下文窗口限制

模型能处理的输入长度有限。

**应对策略**：
- 精简输入内容
- 分块处理长文本
- 使用摘要技术

### 知识截止

模型训练完成后无法获取新信息。

**应对策略**：
- 使用工具调用获取实时信息
- 定期更新模型
- 结合搜索引擎

## 实践建议

### 选择合适的模型

| 场景 | 推荐模型 |
|------|----------|
| 简单任务 | 小型开源模型（如 Llama 7B） |
| 复杂推理 | GPT-4 或 Claude |
| 代码生成 | Codex、Claude Code |
| 本地部署 | Llama、Mistral |

### 优化输入质量

1. **明确目标**：清楚说明你想要什么
2. **提供上下文**：给出足够的背景信息
3. **使用示例**：通过示例说明期望的输出格式
4. **迭代优化**：根据结果调整提示

### 评估输出质量

- 准确性：信息是否正确
- 相关性：是否回答了问题
- 完整性：是否涵盖所有要点
- 可用性：是否可以直接使用

## 下一步

现在你已经了解了 LLM 的基础知识，让我们学习如何有效地与 LLM 交互：

→ [有效提示工程](02-prompt-engineering.md)
